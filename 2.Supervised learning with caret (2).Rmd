---
title: "Supervised learning with caret"
author: "John D. Lee"
date: "2/18/20"
output: html_notebook
---


```{r}

library(tidyverse)
library(skimr)
library(ggforce)
library(caret) # Tools and common interface to many supervised learning algorithms
library(patchwork) # For combining multiple plots
library(plotROC)
library(pROC)

set.seed(888) # To ensure consistent results from non-deterministic procedures
rm(list = ls()) # Removes all variables


```


##  Simplified machine learning process

* Read, transform, and visualize data 

* Partition into training and test sets

* Partition data into training and test sets </b>

* Pre-process data and select features

* Tune model hyperparameters with cross validation

* Estimate variable importance 

* Assess predictions and model performance with test data

* Compare model performance


## Read, clean, transform, and visualize data
Many machine learning algorithms fail if the data includes any missing values.  Be sure you review the skim results and remove columns that contain variables that have many missing variables (select) and remove rows that contain missing values (filter).

```{r review_data}

wine.df = read_csv("winequality-red.csv")
colnames(wine.df) <- make.names(colnames(wine.df), unique=TRUE) # Ensures names are valid and don't contain spaces

skim(wine.df)

head(wine.df)

tail(wine.df)

```


## Define class variable--Good or Bad wine 
```{r define-class, echo = TRUE, warning=FALSE, message=FALSE, out.width="60%"}

quality.wine.df = wine.df %>% mutate(goodwine = if_else(quality > 5, "good", "bad")) %>% 
  mutate(goodwine = as.factor(goodwine))

ggplot(quality.wine.df, aes(goodwine, quality, colour = goodwine, fill = goodwine)) +
  geom_sina(size = .5, alpha = .7, position = position_jitter(height = 0.1)) +
  labs(x = "Discretized wine quality", y = "Rated wine quality") +
  theme(legend.position = "none")

```


## Partition data into training and testing
```{r partition}

wine.df = quality.wine.df %>% select(-quality) # Remove numeric indicator of quality

## Creates a random sample of rows for training
inTrain = createDataPartition(wine.df$goodwine, p = 3/4, list = FALSE) 

## Create dataframes of descriptive variables for training and testing
# Slice extracts rows based on vector of row numbers
trainDescr = wine.df %>% slice(inTrain) %>% select(-goodwine)
testDescr = wine.df %>% slice(-inTrain) %>% select(-goodwine)

trainClass = wine.df %>% slice(inTrain) %>% select(goodwine) %>% as.matrix() %>% as.factor()
testClass = wine.df %>% slice(-inTrain) %>% select(goodwine) %>% as.matrix() %>% as.factor()

## Proportion of good and bad cases should be the same in testing and training
# Ideally the classes should be balanced
wine.df %>% select(goodwine) %>%  table() %>% prop.table() %>% round(3)*100 

trainClass %>% table() %>% prop.table() %>% round(3)*100

testClass %>% table() %>% prop.table() %>% round(3)*100 

```


## Pre-process data: Filter poor predictors, select good predictors, and scale
This function can has many powerful capabilities, including:

* Eliminate variables that have no variabilty and those that are highly correlated 

* Select highly predictive features

* Engineer features by scaling and transformation

* ``preProcess`` creates a transformation function that can be applied to new data

  * ``center`` subtracts mean

  * ``scale`` normalizes based on standard deviation

  *  ``preProcess`` supports other preprocessing methods, such as PCA and ICA

```{r pre-process, cache=FALSE, warning=FALSE, message=FALSE}

## Trans.mod is a transformation model that is trained and the applied to the data
Trans.mod = preProcess(trainDescr, method = c("center", "scale")) 
trainScaled = predict(Trans.mod, trainDescr)
testScaled = predict(Trans.mod, testDescr)

## Plot transformed data
raw.plot = ggplot(trainDescr, aes(density)) + geom_histogram(bins = 60) +
  labs(title = "Original")

scaled.plot = ggplot(trainScaled, aes(density)) + geom_histogram(bins = 60) +
  labs(title = "Scaled")

(raw.plot / scaled.plot) # Using patchwork package

```


## Tune model hyperparameters with cross-validation
* Used to select best combination model parameters

* Estimates model performance (e.g., AUC or r-square) for each candidate model

* Uses a subset of the training data to train the model and a withheld subset to test

The minimal requirements of this function include:
* Select cross validation method: 10-fold repeated cross validation is common

* Define hyperparameter selection method: grid search is the simplest approach

* Define summary measures

* ``trainControl`` command specifies all these parameters in a single statement


## Define training parameters: ``trainControl``
```{r tune}

train.control = trainControl(method = "repeatedcv", 
                             number = 10, repeats = 3, # number: number of folds
                             search = "grid", # for tuning hyperparameters
                             classProbs = TRUE, # return probability of prediction
                             savePredictions = "final",
                             summaryFunction = twoClassSummary
                             )


trainScaled
```


## Select models to train
-- Over 200 different models from 50 categories (e.g., Linear regression, boosting, bagging, cost-sensitive learning)

* List of models: http://caret.r-forge.r-project.org/modelList.html

* The ``train`` statement can train any of them

* Here we select three:

    * Logistic regression

    * Support vector machine

    * xgboost, a boosted random forest that performs well in many situations


## Train models and tune hyperparameters with the ``train`` function

* Specify class and predictor variables

* Specify one of the over 200 models (e.g., xgboost)

* Specify the metric, such as ROC

* Include the ``train.control`` object specified earlier


## Train models and tune hyperparameters: Logistic regression

* Logistic regression has no tuning parameters

* 10-fold repeated 3 times cross-validation occurs once 

* Produces a total of 30 instances of model fitting and testing

* Cross validation provides a nearly unbiased estimate of the performance of the model on the held out data

```{r train_glm, cache = TRUE, echo=TRUE, warning=FALSE}

glm.fit = train(x = trainScaled, y = trainClass,
   method = 'glm', metric = "ROC",
   trControl = train.control) 

glm.fit

```


## Train models and tune hyperparameters: Support vector machine

* Linear support vector machines have a single tuning parameter--C

* C (Cost) 

* C = 1000 "hard margin" tends to be sensitive to individual data points and is prone to over fitting

```{r train_svm, cache=TRUE, message=FALSE, warning=FALSE}

grid = expand.grid(C = c(.1, .2, .4, 1, 2, 4))

svm.fit =  train(x = trainScaled, y = trainClass,
  method = "svmLinear", metric = "ROC",
  tuneGrid = grid, # Overrides tuneLength
  tuneLength = 3, # Number of levels of each hyper parameter, unless specified by grid
  trControl = train.control, scaled = TRUE)

plot(svm.fit)

```


## Train model and tune hyperparameters: xgboost
Classification depends on adding outcomes across many trees

Trees are built in sequence to address the errors (residuals) of the previous trees


* Hyperparameters of xgboost

 * nrounds (# Boosting Iterations)--model robustness

 * max_depth (Max Tree Depth)--model complexity

 * eta (Shrinkage)--model robustness

 * gamma (Minimum Loss Reduction)--model complexity

 * colsample_bytree (Subsample Ratio of Columns)--model robustness

 * min_child_weight (Minimum Sum of Instance Weight)--model complexity

 * subsample (Subsample Percentage)--model robustness

**A grid search with 3 levels for each parameter produces 3^7 combinations!**


``tuneLength = 3`` produces 

    - nrounds (# Boosting Iterations) (50 100 150)

    - max_depth (Max Tree Depth) (1, 2, 3)

    - eta (Shrinkage) (.3, .4)

    - gamma (Minimum Loss Reduction) (0)

    - colsample_bytree (Subsample Ratio of Columns) (.6, .8)

    - min_child_weight (Minimum Sum of Instance Weight) (1)

    - subsample (Subsample Percentage) (.50, .75, 1.0)

-- 108 different model combinations each trained and tested 10X3 times-->3,240 model fits


```{r train_xgb, cache=TRUE, warning=FALSE, message=FALSE}

xgb.fit = train(x = trainScaled, y = trainClass,
  method = "xgbTree", metric = "ROC",
  tuneLength = 3, # Depends on number of parameters in algorithm
  trControl = train.control, scaled = TRUE)

plot(xgb.fit)

```

  
## Assess performance: Confusion matrix (glm)
Use trained model to predict on the4 unseen test data

```{r assess-glm}

glm.pred = predict(glm.fit, testScaled) 

confusionMatrix(glm.pred, testClass)

```


## Assess performance: Confusion matrix (svm)
```{r assess-svm}

svm.pred = predict(svm.fit, testScaled)

confusionMatrix(svm.pred, testClass)

```


## Assess Performance: Confusion matrix (xgb)
```{r assess-xgb}

xgb.pred = predict(xgb.fit, testScaled)

confusionMatrix(xgb.pred, testClass)

```


## Compare models
This function resamples the 10-fold cross validation outcome for the best model parameters.

```{r compare_boxplot, cache=TRUE}

mod.resamps = resamples(list(glm = glm.fit, svm = svm.fit, xgb = xgb.fit))

# dotplot(mod.resamps, metric="ROC")

bwplot(mod.resamps, metric = "ROC")

```


## Assess performance (xgb): ROC plot
The ROC plot provides a more detailed comparison of models across the of decision thresholds.

```{r assess_ROC, warning=FALSE, message= FALSE}

## Use model to generate predictions
xgb.pred = predict(xgb.fit, testScaled, type = "prob")
glm.pred = predict(glm.fit, testScaled, type = "prob")

## Add prediction and observed to test predictors
predicted.wine.df = quality.wine.df %>% slice(-inTrain) %>% 
  cbind(glm.pred.good = glm.pred$good) %>% 
  cbind(xgb.pred.good = xgb.pred$good) %>% 
  cbind(obs = testClass)

## Calculate ROC coordinates and area under curve (AUC)
glm.roc = roc(predictor = predicted.wine.df$glm.pred, 
              response = predicted.wine.df$obs, 
              AUC = TRUE, ci = TRUE)

xgb.roc = roc(predictor = predicted.wine.df$xgb.pred, 
              response = predicted.wine.df$obs, 
              AUC = TRUE, ci = TRUE)

glm.roc$auc
glm.roc$ci


## Plot ROC
xgb_glm.roc.plot = 
ggplot(data = predicted.wine.df, aes(d = obs, m = glm.pred.good)) + 
  geom_abline(colour = "grey60") +
  geom_roc(labels = FALSE, linealpha = .5, pointalpha = .5) + # Labels show the predictor value
  geom_roc(aes(d = obs, m = xgb.pred.good),
           labels = FALSE, linealpha = .8, pointalpha = .8) + # Labels show the predictor value
   annotate("text", x = .5, y = .475, hjust = 0,
           label = paste("AUC(xbg) =", round(xgb.roc$auc, 2))) +
   annotate("text", x = .5, y = .375, hjust = 0,
           label = paste("AUC(glm) =", round(glm.roc$auc, 2))) +
  labs(title = "Prediction of good and bad wines", 
       subtitle = "Extreme gradient boosting predictions (xgboost)") +
  coord_equal() +
  style_roc() 
  
xgb_glm.roc.plot
ggsave("xgb_glm-roc.png", xgb_glm.roc.plot, width = 5, height = 4.5)


```


## Compare xgboost predictions with observed ratings of wine quality
```{r xgb-pred-plot}

predicted_wine.plot = 
  ggplot(predicted.wine.df, aes(as.factor(quality), xgb.pred.good, colour = goodwine)) + 
  geom_sina(size = .5) +
  labs(title = "Prediction of good and bad wines", 
       subtitle = "Extreme gradient boosting predictions (xgboost)",
       x = "Rated quality",
       y = "Predicted probabilty the wine is good") +
  theme_gray(base_size = 14) +
  theme(legend.position = "bottom") 
predicted_wine.plot

ggsave(filename = "predicted_wine.png", plot = predicted_wine.plot, 
       width = 5, height = 4.5)

```

